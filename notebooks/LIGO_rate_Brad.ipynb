{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIGO Event Rate\n",
    "\n",
    "Here, we estimate the LIGO observed merger rate, given a distribution of PBH binaries.\n",
    "\n",
    "**To-Do:** \n",
    "   - Implement and check 'first remapping' calculation\n",
    "   - Write and upload a .py module for perfoming the 'second remapping'\n",
    "   - Write a generic function for calculating the merger rate as a function of (f, M_PBH), with and without halo\n",
    "   - Compare the 'without Halo' results with the Kamionkowski paper\n",
    "\n",
    "\n",
    "**References:** [arXiv:1602.03842](https://arxiv.org/pdf/1602.03842.pdf), [arXiv:1606.03939](https://arxiv.org/abs/1606.03939), [arXiv:1704.04628](https://arxiv.org/abs/1704.04628)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "import matplotlib as mpl\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "#----- MATPLOTLIB paramaters ---------\n",
    "mpl.rcParams.update({'font.size': 18,'font.family':'serif'})\n",
    "\n",
    "mpl.rcParams['xtick.major.size'] = 7\n",
    "mpl.rcParams['xtick.major.width'] = 1\n",
    "mpl.rcParams['xtick.minor.size'] = 3\n",
    "mpl.rcParams['xtick.minor.width'] = 1\n",
    "mpl.rcParams['ytick.major.size'] = 7\n",
    "mpl.rcParams['ytick.major.width'] = 1\n",
    "mpl.rcParams['ytick.minor.size'] = 3\n",
    "mpl.rcParams['ytick.minor.width'] = 1\n",
    "#--------------------------------------\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.integrate import quad\n",
    "\n",
    "\n",
    "import emcee\n",
    "import Remapping as RM\n",
    "reload(RM)\n",
    "\n",
    "import Sampling\n",
    "reload(Sampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIGO sensitivity as a function of redshift\n",
    "\n",
    "Read in the time-volume sensitivity for LIGO, from Fig. 7 of [arXiv:1704.04628](https://arxiv.org/abs/1704.04628):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sens_data_10Msun = np.loadtxt(\"data/LIGO_sensitivity_10.txt\")\n",
    "sens_data_20Msun = np.loadtxt(\"data/LIGO_sensitivity_20.txt\")\n",
    "sens_data_40Msun = np.loadtxt(\"data/LIGO_sensitivity_40.txt\")\n",
    "\n",
    "sens_data_100Msun = np.loadtxt(\"data/LIGO_sensitivity_100.txt\")\n",
    "sens_data_200Msun = np.loadtxt(\"data/LIGO_sensitivity_200.txt\")\n",
    "sens_data_300Msun = np.loadtxt(\"data/LIGO_sensitivity_300.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do some interpolation to estimate the sensitivity for $M_\\mathrm{PBH} = 30\\,M_\\odot$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sens_10Msun = interp1d(sens_data_10Msun[:,0], sens_data_10Msun[:,1], bounds_error=False, fill_value=0.0)\n",
    "sens_20Msun = interp1d(sens_data_20Msun[:,0], sens_data_20Msun[:,1], bounds_error=False, fill_value=0.0)\n",
    "sens_40Msun = interp1d(sens_data_40Msun[:,0], sens_data_40Msun[:,1], bounds_error=False, fill_value=0.0)\n",
    "\n",
    "sens_100Msun = interp1d(sens_data_100Msun[:,0], sens_data_100Msun[:,1], bounds_error=False, fill_value=0.0)\n",
    "sens_200Msun = interp1d(sens_data_200Msun[:,0], sens_data_200Msun[:,1], bounds_error=False, fill_value=0.0)\n",
    "sens_300Msun = interp1d(sens_data_300Msun[:,0], sens_data_300Msun[:,1], bounds_error=False, fill_value=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zvals = np.linspace(0, 0.7, 100)\n",
    "\n",
    "sensitivities = [sens_10Msun, sens_20Msun, sens_40Msun,\\\n",
    "                     sens_100Msun, sens_200Msun, sens_300Msun]\n",
    "labels = [r\"$10\\,M_\\odot$\",r\"$20\\,M_\\odot$\",r\"$40\\,M_\\odot$\",\\\n",
    "              r\"$100\\,M_\\odot$\",r\"$200\\,M_\\odot$\",r\"$300\\,M_\\odot$\"]\n",
    "\n",
    "pl.figure(figsize=(6.1,6))\n",
    "\n",
    "for sens, lab in zip(sensitivities, labels):\n",
    "    pl.plot(zvals, sens(zvals), linewidth=1.5, label=lab)\n",
    "    \n",
    "pl.xlim(0,0.75)\n",
    "pl.ylim(0, 0.75)\n",
    "    \n",
    "pl.xlabel(r\"$z$\")\n",
    "pl.ylabel(r\"$\\mathrm{d}\\langle V T \\rangle/\\mathrm{d}z$ (Gpc$^3$ yr)\")\n",
    "\n",
    "pl.legend(loc='upper right', frameon=False)\n",
    "\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redshift-time relation\n",
    "\n",
    "The time since the Big Bang is given by:\n",
    "\n",
    "$$t(z) = \\int_z^\\infty \\frac{\\mathrm{d}z'}{(1+z') H(z')}\\,,$$\n",
    "\n",
    "where\n",
    "\n",
    "$$H(z') = \\sqrt{\\Omega_\\Lambda + \\Omega_m (1+z)^3}\\,,$$\n",
    "\n",
    "in a flat universe. We use the Planck 2015 values from [arXiv:1502.01589](https://arxiv.org/abs/1502.01589), noting that there is some tension between different measurements of $H_0$:\n",
    "\n",
    "- $H_0 = 67.8 \\,\\,\\mathrm{km/s}\\,\\mathrm{Mpc}^{-1}$\n",
    "- $\\Omega_\\Lambda = 0.692$\n",
    "- $\\Omega_m = 0.308$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get H in units of 1/yr\n",
    "#(km/Mpc) = 3.24e-20\n",
    "H0_peryr = 67.8*(3.24e-20)*(60*60*24*365)\n",
    "\n",
    "Omega_L = 0.692\n",
    "Omega_m = 0.308\n",
    "\n",
    "def Hubble(z):\n",
    "    return H0_peryr*np.sqrt(Omega_L + Omega_m*(1+z)**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate time from Big Bang until redshift z (in years)\n",
    "def t_univ(z):\n",
    "    integ = lambda x: 1.0/((1+x)*Hubble(x))\n",
    "    return quad(integ, z, np.inf)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot age of the universe as a function of Redshift\n",
    "\n",
    "zlist = np.linspace(0, 2, 50)\n",
    "tvals = np.asarray([t_univ(z) for z in zlist])\n",
    "\n",
    "pl.figure(figsize=(6.1,6))\n",
    "pl.plot(zlist, tvals/1e9)\n",
    "\n",
    "pl.xlabel(r\"Redshift, $z$\")\n",
    "pl.ylabel(r\"Time since Big Bang, $t$ [Gyr]\")\n",
    "\n",
    "pl.ylim(0, 15)\n",
    "\n",
    "pl.show()\n",
    "\n",
    "z_of_t = interp1d(tvals, zlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now also plot z as a function of time\n",
    "\n",
    "pl.figure(figsize=(6.1,6))\n",
    "pl.plot(tvals/1e9,zlist)\n",
    "\n",
    "pl.axhline(0.6, linestyle='--', color='k')\n",
    "\n",
    "pl.ylabel(r\"Redshift, $z$\")\n",
    "pl.xlabel(r\"Time since Big Bang, $t$ [Gyr]\")\n",
    "\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merger Rate\n",
    "\n",
    "Calculate Merger rate from PBH binaries, as a function of the merger time. This is given by:\n",
    "\n",
    "$$ R_\\mathrm{merge}(t_\\mathrm{merge}) = n_\\mathrm{PBH} P(t_\\mathrm{merge})\\,.$$\n",
    "\n",
    "Here, $n_\\mathrm{PBH}$ is the number density of PBHs and $P(t_\\mathrm{merge})$ is the PDF for the merger time of the binaries. Note that not all PBHs will form binaries, so we choose to normalise $P(t_\\mathrm{merge})$ to the number of binaries which form *per PBH*:\n",
    "\n",
    "$$\\int P(t_\\mathrm{merge}) \\,\\mathrm{d}t_\\mathrm{merge} = \\frac{n_\\mathrm{binaries}}{n_\\mathrm{PBH}}\\,.$$\n",
    "\n",
    "From this, the maximum normalisation is obtained when all PBHs form binaries, i.e. $n_\\mathrm{binaries}/n_\\mathrm{PBH} = 1/2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate density of PBH binaries\n",
    "\n",
    "#Fraction of DM made of PBHs\n",
    "#f = 4e-3\n",
    "#M_PBH = 20.0 #Solar masses\n",
    "\n",
    "#Some cosmological parameters\n",
    "G_N = 4.302e-3 #(pc/solar mass) (km/s)^2\n",
    "\n",
    "h = 0.678\n",
    "H0 = 100.0*h #(km/s) Mpc^-1\n",
    "Omega_DM = 0.1186/(h**2)\n",
    "    \n",
    "def calc_nPBH(f, M_PBH):\n",
    "\n",
    "\n",
    "    Omega_PBH = f*Omega_DM\n",
    "    rho = 3.0*H0**2/(8.0*np.pi*(G_N*1e-6)) #Solar masses per Mpc^3\n",
    "\n",
    "    n_PBH = (1e3)**3*rho*Omega_PBH/M_PBH #PBH per Gpc^3\n",
    "    \n",
    "    return n_PBH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define + calculate merger time\n",
    "\n",
    "#Variance of DM density perturbations at equality\n",
    "sigma_eq = 0.005\n",
    "\n",
    "#Volume factor for converting from Sasaki to Kamionkowski\n",
    "volfac = (3.0/(4.0*np.pi))**(1.0/3.0)\n",
    "alpha = 0.1\n",
    "\n",
    "lambda_max = 3.0\n",
    "\n",
    "def calcj(e):\n",
    "    return np.sqrt(1-e**2)\n",
    "\n",
    "#Mean PBH separation at z_eq (in pc)\n",
    "rho_eq = 1512.0 #Solar masses per pc^3\n",
    "def xbar_S(f, M_PBH):\n",
    "    return 0.28*((M_PBH/30.0)**(1.0/3.0))*(f**(-1.0/3.0))\n",
    "\n",
    "def xbar(f, M_PBH):\n",
    "    return (3.0*M_PBH/(4*np.pi*rho_eq*(0.85*f)))**(1.0/3.0)\n",
    "\n",
    "#Truncation radius of UCMH at z = z_eq:\n",
    "def r_eq(M_PBH=30.0):\n",
    "    return 1.96e-2*(M_PBH/30.0)**(1.0/3.0)\n",
    "\n",
    "def M_halo(a, M_PBH=30.0):\n",
    "    return M_PBH*(rtr_interp(a)/r_eq(M_PBH))**1.5\n",
    "\n",
    "def density(r):\n",
    "    if (r > r_cut):\n",
    "        return 0\n",
    "    else:\n",
    "        return A*(r/r_tr)**(-3.0/2.0)\n",
    "    \n",
    "def Menc(r):\n",
    "    if (r > r_cut):\n",
    "        return M_PBH*(1+(r_cut/r_tr)**(3/2))\n",
    "    else:\n",
    "        return M_PBH*(1+(r/r_tr)**(3/2))    \n",
    "\n",
    "def bigX(x, f, M_PBH):\n",
    "    return (x/(xbar(f,M_PBH)))**3.0\n",
    "\n",
    "def a_of_x(x, f, M_PBH):\n",
    "    \n",
    "    xb = xbar(f, M_PBH)\n",
    "    return (alpha/(0.85*f))*x**4/xb**3\n",
    "\n",
    "def x_of_a(a, f, M_PBH, withHalo = False):\n",
    "    \n",
    "    xb = xbar(f, M_PBH)\n",
    "    \n",
    "    if (not withHalo):\n",
    "        \n",
    "        return (a*f*0.85*xb**3/alpha)**(1.0/4.0)\n",
    "    \n",
    "    elif (withHalo):\n",
    "                                                              \n",
    "        xb_rescaled = xb * ((M_PBH + M_halo(a, M_PBH))/M_PBH )**(1./3.)            \n",
    "        return (a*f*0.85*xb_rescaled**3/alpha)**(1.0/4.0)\n",
    " \n",
    "#Distribution of j\n",
    "def j_X(x, f, M_PBH):\n",
    "    return bigX(x, f, M_PBH)*0.5*(1+sigma_eq**2/(0.85*f)**2)**0.5\n",
    "\n",
    "def P_j(j, x, f, M_PBH):\n",
    "    y = j/j_X(x, f, M_PBH)\n",
    "    return (y**2/(1+y**2)**(3.0/2.0))/j\n",
    "\n",
    "#Maximum semi-major axis\n",
    "def a_max(f, M_PBH):\n",
    "    return alpha*xbar(f, M_PBH)*(f*0.85)**(1.0/3.0)*((lambda_max)**(4.0/3.0))\n",
    "\n",
    "\n",
    "\n",
    "def P_a_j(a, j, f=1e-2, M_PBH=30.0):\n",
    "    xval = x_of_a(a, f, M_PBH)\n",
    "    X = bigX(xval, f, M_PBH)\n",
    "    xb = xbar(f, M_PBH)\n",
    "    measure = (3.0/4.0)*(a**-0.25)*(0.85*f/(alpha*xb))**0.75\n",
    "    return P_j(j, xval, f, M_PBH)*np.exp(-X)*measure\n",
    "\n",
    "P_a_j_vec = np.vectorize(P_a_j, excluded=(2,3))\n",
    "\n",
    "def P_a_j_withHalo(a, j, f=1e-2, M_PBH=30.0):\n",
    "    xval = x_of_a(a, f, M_PBH, withHalo = True)\n",
    "    X = bigX(xval, f, M_PBH)\n",
    "    xb = xbar(f, M_PBH)\n",
    "    \n",
    "    rm = ((M_PBH + M_halo(a, M_PBH))/M_PBH)\n",
    "    \n",
    "    measure = (3.0/4.0)*(a**-0.25)*(0.85*f/(alpha*xb))**0.75\n",
    "    return P_j(j, xval, f, M_PBH)*np.exp(-X)*measure*rm**(3./4.)\n",
    "\n",
    "def P_a(a, f, M_PBH):\n",
    "    xval = x_of_a(a, f, M_PBH)\n",
    "    X = bigX(xval, f, M_PBH)\n",
    "    xb = xbar(f, M_PBH)\n",
    "    measure = (3.0/4.0)*(a**-0.25)*(0.85*f/(alpha*xb))**0.75\n",
    "    return np.exp(-X)*measure\n",
    "\n",
    "\n",
    "def P_a_withHalo(a, f, M_PBH):\n",
    "    xval = x_of_a(a, f, M_PBH, withHalo = True)\n",
    "    X = bigX(xval, f, M_PBH)\n",
    "    xb = xbar(f, M_PBH)\n",
    "    measure = (3.0/4.0)*(a**-0.25)*(0.85*f/(alpha*xb))**0.75\n",
    "    return np.exp(-X)*measure\n",
    "\n",
    "def P_la_lj(la,lj,f=1e-2, M_PBH=30.0):\n",
    "    j = 10**lj\n",
    "    a = 10**la\n",
    "    return P_a_j(a, j, f, M_PBH)*a*j*(np.log(10)**2)\n",
    "\n",
    "def P_la_lj_withHalo(la, lj, f=1e-3, M_PBH=30.0):\n",
    "    j = 10**lj\n",
    "    a = 10**la\n",
    "    return P_a_j_withHalo(a, j, f, M_PBH)*a*j*(np.log(10)**2)\n",
    "\n",
    "def t_coal(a, e, M_PBH=30.0):\n",
    "    Q = (3.0/170.0)*(G_N*M_PBH)**-3 # s^6 pc^-3 km^-6\n",
    "    tc = Q*a**4*(1-e**2)**(7.0/2.0) #s^6 pc km^-6\n",
    "    tc *= 3.086e+13 #s^6 km^-5\n",
    "    tc *= (3e5)**5 #s\n",
    "    return tc/(60*60*24*365) #in years\n",
    "\n",
    "def j_coal(a, t, M_PBH=30.0):\n",
    "    Q = (3.0/170.0)*(G_N*M_PBH)**-3 # s^6 pc^-3 km^-6\n",
    "    tc = t*(60*60*24*365)\n",
    "    tc /= (3e5)**5\n",
    "    tc /= 3.086e+13\n",
    "    return (tc/(Q*a**4))**(1.0/7.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P_t(t, f, M_PBH, withHalo = False):\n",
    "    amin = 1e-6\n",
    "    if (withHalo):\n",
    "        amax = a_max(f, 2.0*M_PBH)\n",
    "    else:\n",
    "        amax = a_max(f, M_PBH)\n",
    "\n",
    "    alist = np.logspace(np.log10(amin), np.log10(amax), 100)\n",
    "    Plist = 0.0*alist\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i, a in enumerate(alist):\n",
    "        M_tot = 1.0*M_PBH\n",
    "        #if (withHalo):\n",
    "        #    M_tot += M_halo(a, M_PBH)\n",
    "        \n",
    "        j = j_coal(a, t, M_tot)\n",
    "        if (j >= 1):\n",
    "            Plist[i] = 0\n",
    "            \n",
    "        else:\n",
    "            e = np.sqrt(1-j**2)\n",
    "            djde = e/j\n",
    "            dedt = (j**2/e)/(7.0*t)\n",
    "            \n",
    "            if (withHalo):\n",
    "                Plist[i] = P_a_j_withHalo(a, j, f, M_PBH)*djde*dedt\n",
    "            else:\n",
    "                Plist[i] = P_a_j(a, j, f, M_PBH)*djde*dedt\n",
    "        \n",
    "    #pl.figure()\n",
    "    #pl.loglog(alist, Plist)\n",
    "    #pl.show()\n",
    "    \n",
    "    return np.trapz(Plist, alist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merger_rate(t_merge, f, M_PBH, withHalo=False):\n",
    "    return calc_nPBH(f, M_PBH)*P_t(t_merge, f, M_PBH, withHalo) #Mergers per Gpc^3 per year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_ref = 1e-2\n",
    "M_PBH = 30\n",
    "\n",
    "a_list = np.logspace(np.log10(1e-6), np.log10(a_max(f=f_ref,M_PBH=2*30.0)),100)\n",
    "\n",
    "j_list = np.asarray([j_coal(a, 13.5e9, M_PBH=30.0) for a in a_list])\n",
    "\n",
    "\n",
    "P_a_j_vec = np.vectorize(P_a_j, excluded=(2,3))\n",
    "P_a_j_withHalo_vec = np.vectorize(P_a_j_withHalo, excluded=(2,3))\n",
    "P_list = P_a_j_vec(a_list, j_list, f_ref, M_PBH)\n",
    "P_withHalo_list = P_a_j_withHalo_vec(a_list, j_list, f_ref, M_PBH)\n",
    "\n",
    "a_peak = a_list[np.argmax(P_list)]\n",
    "\n",
    "print j_coal(a_peak, 13.5e9, M_PBH)\n",
    "print \"Most likely value of a [pc]:\", a_peak\n",
    "print \"Truncation radius [pc]:\", rtr_interp(a_peak)\n",
    "print \"Req. eccentricity:\", np.sqrt(1-(j_coal(a_peak, 13.5e9, M_PBH))**2)\n",
    "print \"Halo Mass [Msolar]:\", M_halo(a_peak, M_PBH)\n",
    "\n",
    "pl.figure()\n",
    "pl.plot(a_list, P_list, label='Without Halo')\n",
    "pl.plot(a_list, P_withHalo_list, label='With Halo')\n",
    "pl.xlabel(\"Semi-major axis [pc]\")\n",
    "pl.ylabel(r\"$P(a|t_\\mathrm{merge} = t_\\mathrm{univ})$ [a.u.]\")\n",
    "\n",
    "pl.axvline(a_max(f=f_ref,M_PBH=30.0), linestyle='--', color='k')\n",
    "\n",
    "pl.legend()\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvals = np.logspace(np.log10(1e9), np.log10(2e10))\n",
    "\n",
    "pl.figure()\n",
    "pl.plot(np.log10(tvals), tvals*np.log(10)*np.vectorize(P_t)(tvals, f, M_PBH, withHalo = False))\n",
    "pl.ylim(0, 0.002)\n",
    "pl.show()\n",
    "\n",
    "pl.figure()\n",
    "pl.plot(tvals, np.vectorize(P_t)(tvals, f, M_PBH, withHalo = False))\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIGO Event Rate\n",
    "\n",
    "Calculate number of events above threshold in the search presented in [arXiv:1602.03842](https://arxiv.org/pdf/1602.03842.pdf):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate merger rate as a function of z\n",
    "def merger_rate_z(z, f, M_PBH,withHalo=False):\n",
    "    return merger_rate(t_univ(z), f, M_PBH, withHalo)\n",
    "\n",
    "\n",
    "#Integrate over sensitivity to get number of events\n",
    "#We don't have the sensitivity curve for 30 Msun, so do 20 Msun and 40 Msun\n",
    "integrand_20 = lambda x: sens_20Msun(x)*merger_rate_z(x, f=1e-2, M_PBH=20.0)\n",
    "integrand_40 = lambda x: sens_40Msun(x)*merger_rate_z(x, f=1e-2, M_PBH=20.0)\n",
    "\n",
    "integrand_20_flat = lambda x: sens_20Msun(x)\n",
    "integrand_40_flat = lambda x: sens_40Msun(x)\n",
    "\n",
    "\n",
    "#Number of merger events above a given threshold in the search presented in \n",
    "N_20 = quad(integrand_20, 0, 0.7)[0]\n",
    "N_40 = quad(integrand_40, 0, 0.7)[0]\n",
    "\n",
    "N_20_flat = quad(integrand_20_flat, 0, 0.7)[0]\n",
    "N_40_flat = quad(integrand_40_flat, 0, 0.7)[0]\n",
    "\n",
    "print \"Number of events above threshold:\", N_20, \"-\", N_40\n",
    "\n",
    "\n",
    "print \"Estimated merger rate [Gpc^-3 yr^-1]:\", N_20/N_20_flat, \"-\", N_40/N_40_flat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 1e-2\n",
    "M_PBH = 30\n",
    "\n",
    "rtr_interp = RM.GetRtrInterp(M_PBH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samps = Sampling.GetSamples_MCMC(50000, P_la_lj_withHalo, 1e-6, a_max(f, 2.0*M_PBH), f, M_PBH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_samps = np.asarray([10**x[0] for x in samps])\n",
    "j_samps = np.asarray([10**x[1] for x in samps])\n",
    "ti_samps = t_coal(a_samps,np.sqrt(1-j_samps**2), M_PBH)\n",
    "\n",
    "af_samps = np.asarray([RM.calc_af(ai,M_PBH) for ai in a_samps])\n",
    "jf_samps = np.asarray([RM.calc_jf(ji, ai,M_PBH) for ji,ai in zip(j_samps,a_samps)])\n",
    "tf_samps = np.asarray([RM.calc_Tf(ti, ai, M_PBH) for ti, ai in zip(ti_samps, a_samps)])\n",
    "\n",
    "#tf_samps = np.asarray([t_coal(10**x[0],np.sqrt(1-10**(2.0*x[1])), M_PBH) for x in samps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "xy = np.vstack([np.log10(a_samps),np.log10(j_samps)])\n",
    "z = gaussian_kde(xy)(xy)\n",
    "\n",
    "xy2 = np.vstack([np.log10(af_samps),np.log10(jf_samps)])\n",
    "z2 = gaussian_kde(xy2)(xy2)\n",
    "\"\"\"\n",
    "\n",
    "pl.figure()\n",
    "\n",
    "pl.scatter(np.log10(a_samps),np.log10(j_samps),alpha = 0.01)\n",
    "pl.scatter(np.log10(af_samps),np.log10(jf_samps),alpha = 0.01)\n",
    "#pl.scatter(np.log10(af_samps),np.log10(jf_samps), c=z2, s=100, edgecolor='', alpha= 0.1)\n",
    "#pl.scatter(np.log10(a_samps), np.log10(j_samps), alpha=0.1)\n",
    "#pl.scatter(np.log10(af_samps), np.log10(jf_samps), alpha=0.1)\n",
    "#pl.scatter(np.log10(a_samps), np.log10(tf_samps), alpha=0.1)\n",
    "#pl.colorbar()\n",
    "pl.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print np.min(ti_samps/1e9)\n",
    "print np.min(tf_samps/1e9)\n",
    "\n",
    "N_tot = 1.0*len(a_samps)\n",
    "\n",
    "bins = np.logspace(7, 11, 101)\n",
    "lbins = np.linspace(7, 11, 101)\n",
    "#bins = np.linspace(1e7, 1e11, 1001)\n",
    "bin_centres = np.sqrt(bins[:-1]*bins[1:])\n",
    "\n",
    "pl.figure()\n",
    "n_f, bins, patches  = pl.hist(np.log10(tf_samps), bins=lbins, normed=True, alpha=0.75)\n",
    "n_i, bins, patches  = pl.hist(np.log10(ti_samps), bins=lbins, normed=True, alpha=0.75)\n",
    "\n",
    "\n",
    "pl.axvline(np.log10(t_univ(0.0)))\n",
    "pl.axvline(np.log10(t_univ(1.0)))\n",
    "\n",
    "pl.xlim(9, np.log10(20e9))\n",
    "\n",
    "pl.xlabel(r'$\\log_{10}(t/\\mathrm{yr})$')\n",
    "pl.ylabel('Counts')\n",
    "\n",
    "pl.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bin_centres = np.sqrt(bins[:-1]*bins[1:])\n",
    "\n",
    "t1 = t_univ(0.7)\n",
    "t2 = t_univ(0.0)\n",
    "print t1/1e9,t2/1e9\n",
    "\n",
    "\n",
    "tvals = np.logspace(8, 11, 1000)\n",
    "\n",
    "\n",
    "P_true = np.asarray([P_t(t, f, M_PBH, withHalo=False) for t in tvals])\n",
    "P_true_withHalo = np.asarray([P_t(t, f, M_PBH, withHalo=True) for t in tvals])\n",
    "\n",
    "pl.figure()\n",
    "\n",
    "pl.loglog(tvals, P_true, label=\"Without halo\")\n",
    "pl.loglog(tvals, P_true_withHalo, label=\"With halo\")\n",
    "\n",
    "pl.legend()\n",
    "pl.show()\n",
    "\n",
    "#P_norm = np.trapz(P_true*1.0/(bin_centres*np.log(10)), bin_centres)\n",
    "P_norm = np.trapz(P_true, tvals)\n",
    "print P_norm\n",
    "P_norm_withHalo = np.trapz(P_true_withHalo, tvals)\n",
    "print P_norm_withHalo\n",
    "\n",
    "ni_normed = n_i/(bin_centres*np.log(10))\n",
    "nf_normed = n_f/(bin_centres*np.log(10))\n",
    "\n",
    "#norm = np.trapz(ni_normed, bin_centres)\n",
    "norm = 1.0\n",
    "\n",
    "ni_normed *= P_norm_withHalo/norm\n",
    "nf_normed *= P_norm_withHalo/norm\n",
    "\n",
    "\n",
    "\n",
    "pl.figure()\n",
    "pl.step(bin_centres, ni_normed, where='mid', label='Initial')\n",
    "pl.step(bin_centres, nf_normed, where='mid', label='Final')\n",
    "#pl.loglog(bin_centres, (1.0/P_norm)*P_true*1.0/(bin_centres*np.log(10)), '+')\n",
    "pl.plot(tvals, P_true, '-')\n",
    "\n",
    "pl.xlabel(r\"$t_\\mathrm{merge} \\, [yr]$\")\n",
    "pl.ylabel(r\"$P(t_\\mathrm{merge}) [\\mathrm{yr}^{-1}]$\")\n",
    "\n",
    "pl.axvline(t1, linestyle='--', color='k')\n",
    "pl.axvline(t2, linestyle='--', color='k')\n",
    "\n",
    "pl.xlim(1e9, 50e9)\n",
    "\n",
    "pl.legend(loc='best')\n",
    "\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print t1, t2\n",
    "\n",
    "P_ti_interp = interp1d(bin_centres, ni_normed, kind='linear')\n",
    "P_tf_interp = interp1d(bin_centres, nf_normed, kind='linear')\n",
    "#print quad(lambda y: P_lt_interp(y), 1.5e8, 0.9e11)[0]\n",
    "#print quad(lambda y: P_lt_interp(y), t1, t2)[0]\n",
    "\n",
    "def merger_rate_i(z, f, M_PBH):\n",
    "    return P_ti_interp(t_univ(z))*calc_nPBH(f, M_PBH)\n",
    "\n",
    "def merger_rate_f(z, f, M_PBH):\n",
    "    return P_tf_interp(t_univ(z))*calc_nPBH(f, M_PBH)\n",
    "\n",
    "#x = 0.5\n",
    "\n",
    "#print \"R_new/R_old = \", quad(lambda y: P_tf_interp(y), t1, t2)[0]/quad(lambda y: P_ti_interp(y), t1, t2)[0]\n",
    "\n",
    "#Integrate over sensitivity to get number of events\n",
    "#We don't have the sensitivity curve for 30 Msun, so do 20 Msun and 40 Msun\n",
    "\n",
    "N_anal = np.zeros(2)\n",
    "N_i = 0.0*N_anal\n",
    "N_f = 0.0*N_anal\n",
    "N_flat = 0.0*N_anal\n",
    "\n",
    "for ind,sens_curve in enumerate([sens_20Msun, sens_40Msun]):\n",
    "    \n",
    "    integrand = lambda x: sens_curve(x)*merger_rate_z(x, f=1e-2, M_PBH=M_PBH)\n",
    "    integrand_i = lambda x: sens_curve(x)*merger_rate_i(x, f=1e-2, M_PBH=M_PBH)\n",
    "    integrand_f = lambda x: sens_curve(x)*merger_rate_f(x, f=1e-2, M_PBH=M_PBH)\n",
    "\n",
    "    integrand_flat = lambda x: sens_curve(x)\n",
    "    \n",
    "    #Number of merger events above a given threshold in the search presented in \n",
    "    N_anal[ind] = quad(integrand, 0, 0.7)[0]\n",
    "    N_i[ind] = quad(integrand_i, 0, 0.7)[0]\n",
    "    N_f[ind] = quad(integrand_f, 0, 0.7)[0]\n",
    "    #N_40 = quad(integrand_40, 0, 0.7)[0]\n",
    "    N_flat[ind] = quad(integrand_flat, 0, 0.7)[0]\n",
    "\n",
    "print \"M_PBH = \", M_PBH, \"; f = 1e-2\"\n",
    "print \" Initial merger rate (analytic) [Gpc^-3 yr^-1]:\", N_anal[0]/N_flat[0], \"-\", N_anal[1]/N_flat[1]\n",
    "print \" Initial merger rate (sampled) [Gpc^-3 yr^-1]:\", N_i[0]/N_flat[0], \"-\", N_i[1]/N_flat[1]\n",
    "print \" Final merger rate [Gpc^-3 yr^-1]:\",N_f[0]/N_flat[0], \"-\", N_f[1]/N_flat[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate unmapped merger rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fvals = np.logspace(-4, 0)\n",
    "\n",
    "M_PBH = 30.0\n",
    "\n",
    "Rvals = np.asarray([merger_rate_z(0,f, M_PBH) for f in fvals])\n",
    "\n",
    "z_list = np.linspace(0, 0.5, 21)\n",
    "merge_list = np.vectorize(merger_rate_z)(z_list, 1e-2, 30.0)\n",
    "\n",
    "print np.trapz(merge_list, z_list)/(np.max(z_list) - np.min(z_list))\n",
    "\n",
    "pl.figure(figsize=(7,5))\n",
    "\n",
    "for M_PBH in [1, 30, 1000]:\n",
    "    Rvals = np.asarray([merger_rate_z(0,f, M_PBH) for f in fvals])\n",
    "    pl.loglog(fvals, Rvals, label='$'+str(M_PBH)+'\\,M_\\odot$')\n",
    "\n",
    "pl.legend()\n",
    "\n",
    "pl.xlabel('$f_\\mathrm{PBH}$')\n",
    "pl.ylabel('Merger Rate [$\\mathrm{Gpc}^{-3}\\,\\mathrm{yr}^{-1}$]')\n",
    "pl.tight_layout()\n",
    "\n",
    "pl.savefig(\"Mergers_today.pdf\")\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automating the calculation of the merger rate today\n",
    "\n",
    " - Fix this so that I don't need rtr_interp to be global!\n",
    " - Speed up so that it doesn't take so damn long!\n",
    " \n",
    " **Note that $P(\\log_{10}t)$ is reasonably flat, but $P(t)$ is actually not!** Need to be careful about what we mean by 'mergers today'..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the redshift as a function of t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zlist = np.linspace(0, 2.0, 100)\n",
    "tvals = np.asarray([t_univ(z) for z in zlist])\n",
    "\n",
    "z_of_t_interp = interp1d(tvals, zlist)\n",
    "\n",
    "def z_of_t(t):\n",
    "    if ((t > t_univ(0.0)) or (t < t_univ(1.0))):\n",
    "        return 5.0\n",
    "    else:\n",
    "        return z_of_t_interp(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_list = np.asarray([10., 20., 40.])\n",
    "VT = np.zeros_like(M_list)\n",
    "\n",
    "sensitivities = [sens_data_10Msun, sens_data_20Msun, sens_data_40Msun]\n",
    "\n",
    "#Calculate stellar mass BH sensitivities by integrating\n",
    "for i, sens in enumerate(sensitivities):\n",
    "    VT[i] = np.trapz(sens[:,1], sens[:,0])\n",
    "\n",
    "#Calculate IMBH sensitivities by converting from R90    \n",
    "R_90_IMBH = np.loadtxt(\"data/LIGO_R90_IMBH.txt\")\n",
    "M_list = np.append(M_list, R_90_IMBH[:,0])\n",
    "VT_IMBH = np.asarray([2.303/R_90_IMBH[j,1] for j in range(3)]) \n",
    "\n",
    "VT = np.append(VT, VT_IMBH)\n",
    "\n",
    "#Do some plotting\n",
    "pl.figure()\n",
    "pl.loglog(M_list, VT, 's')\n",
    "pl.xlabel(r\"$M_\\mathrm{BH} \\,[M_\\odot]$\")\n",
    "pl.ylabel(r\"$\\langle VT \\rangle \\, [\\mathrm{Gpc}^3 \\,\\mathrm{yr}]$\")\n",
    "pl.title(\"Time-volume sensitivity\", fontsize=14.0)\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DOES THIS METHOD WORK???!?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalcLIGOMergerRate_unmapped(f, M_PBH, sens):\n",
    "    global rtr_interp\n",
    "    #Calculating truncation radius as a function of a\n",
    "    rtr_interp = RM.GetRtrInterp(M_PBH)\n",
    "    #print rtr_interp(0.01)\n",
    "    \n",
    "    #Generating samples from the PBH\n",
    "    samps = Sampling.GetSamples_MCMC(1000, P_la_lj, 1e-6, a_max(f, M_PBH), f, M_PBH)\n",
    "    \n",
    "    #Calculating final merger time\n",
    "    a_samps = np.asarray([10**x[0] for x in samps])\n",
    "    j_samps = np.asarray([10**x[1] for x in samps])\n",
    "    \n",
    "    ti_samps = t_coal(a_samps,np.sqrt(1-j_samps**2), M_PBH)\n",
    "    #tf_samps = np.asarray([RM.calc_Tf(ti, ai, M_PBH) for ti, ai in zip(ti_samps, a_samps)])\n",
    "\n",
    "    N_samps = 1.0*len(ti_samps)\n",
    "    \n",
    "    #Calculate number of binaries merging in the LIGO range 'today'\n",
    "    t1 = t_univ(1.0)\n",
    "    t2 = t_univ(0.0)\n",
    "    #print quad(sens, 0.0, 1.0)[0]\n",
    "    #N_merge = np.sum( np.logical_and(tf_samps > t1,tf_samps < t2))\n",
    "    N_merge = np.sum(sens(np.vectorize(z_of_t)(ti_samps)))/np.sum(ti_samps < t2)\n",
    "\n",
    "    #print \"   Estimate of normalisation of P(t):\", x1\n",
    "    \n",
    "    #But we need to check the normalisation of P(t) in the range t1 -> t2\n",
    "    \n",
    "    #Normalise PDF correctly, to obtain fraction of BHs which form binaries in a given interval of t_merge\n",
    "    tvals = np.logspace(9, 11, 500)\n",
    "    P_true = np.asarray([P_t(t, f, M_PBH, withHalo=False) for t in tvals])\n",
    "    f_bin = np.trapz(P_true, tvals)\n",
    "    \n",
    "    print f_bin\n",
    "    frac = np.sum(ti_samps < t2)/N_samps\n",
    "\n",
    "    #frac = 1.0\n",
    "    z_upper = z_of_t(1e9)\n",
    "    print z_upper\n",
    "    \n",
    "    frac = 1.0\n",
    "    \n",
    "    #Do the calculation in terms of z, to make sure my PDF is correctly normalised!\n",
    "    zvals = np.linspace(0,z_upper,100)\n",
    "    P_true_z = np.asarray([P_t(t_univ(z), f, M_PBH, withHalo=False) for z in zvals])\n",
    "    f_bin_z = f_bin*np.trapz(P_true_z, zvals)/frac\n",
    "\n",
    "\n",
    "    return f_bin_z*N_merge*calc_nPBH(f, M_PBH)/(quad(sens, 0.0, 1.0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalcLIGOMergerRate_unmapped2(f, M_PBH, sens):\n",
    "    global rtr_interp\n",
    "    #Calculating truncation radius as a function of a\n",
    "    rtr_interp = RM.GetRtrInterp(M_PBH)\n",
    "    #print rtr_interp(0.01)\n",
    "    \n",
    "    #Generating samples from the PBH\n",
    "    samps = Sampling.GetSamples_MCMC(10000, P_la_lj, 1e-6, a_max(f, M_PBH), f, M_PBH)\n",
    "    \n",
    "    #Calculating final merger time\n",
    "    a_samps = np.asarray([10**x[0] for x in samps])\n",
    "    j_samps = np.asarray([10**x[1] for x in samps])\n",
    "    \n",
    "    ti_samps = t_coal(a_samps,np.sqrt(1-j_samps**2), M_PBH)\n",
    "    #tf_samps = np.asarray([RM.calc_Tf(ti, ai, M_PBH) for ti, ai in zip(ti_samps, a_samps)])\n",
    "\n",
    "    N_samps = 1.0*len(ti_samps)\n",
    "    print \"   N_samps = \", N_samps\n",
    "    \n",
    "    #Calculate number of binaries merging in the LIGO range 'today'\n",
    "    t1 = t_univ(1.0)\n",
    "    t2 = t_univ(0.0)\n",
    "    #print quad(sens, 0.0, 1.0)[0]\n",
    "    #N_merge = np.sum( np.logical_and(tf_samps > t1,tf_samps < t2))\n",
    "    t_crop = ti_samps[ti_samps < t2]\n",
    "    z_samps = np.vectorize(z_of_t)(t_crop)\n",
    "    N_merge = np.sum(Hubble(z_samps)*(1+z_samps)*sens(z_samps))/N_samps\n",
    "\n",
    "    #print \"   Estimate of normalisation of P(t):\", x1\n",
    "    \n",
    "    #But we need to check the normalisation of P(t) in the range t1 -> t2\n",
    "    \n",
    "    #Normalise PDF correctly, to obtain fraction of BHs which form binaries in a given interval of t_merge\n",
    "    tvals = np.logspace(9, 11, 500)\n",
    "    P_true = np.asarray([P_t(t, f, M_PBH, withHalo=False) for t in tvals])\n",
    "    f_bin = np.trapz(P_true, tvals)\n",
    "\n",
    "    res = f_bin*N_merge*calc_nPBH(f, M_PBH)/(quad(sens, 0.0, 1.0)[0])\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalcLIGOMergerRate(f, M_PBH, sens):\n",
    "    global rtr_interp\n",
    "    #Calculating truncation radius as a function of a\n",
    "    rtr_interp = RM.GetRtrInterp(M_PBH)\n",
    "    #print rtr_interp(0.01)\n",
    "    \n",
    "    #Generating samples from the PBH\n",
    "    samps = Sampling.GetSamples_MCMC(10000, P_la_lj_withHalo, 1e-6, a_max(f, 2*M_PBH), f, M_PBH)\n",
    "    \n",
    "    #Calculating final merger time\n",
    "    a_samps = np.asarray([10**x[0] for x in samps])\n",
    "    j_samps = np.asarray([10**x[1] for x in samps])\n",
    "    \n",
    "    ti_samps = t_coal(a_samps,np.sqrt(1-j_samps**2), M_PBH)\n",
    "    tf_samps = np.asarray([RM.calc_Tf(ti, ai, M_PBH) for ti, ai in zip(ti_samps, a_samps)])\n",
    "\n",
    "    N_samps = 1.0*len(ti_samps)\n",
    "    print \"   N_samps = \", N_samps\n",
    "    \n",
    "    #Calculate number of binaries merging in the LIGO range 'today'\n",
    "    t1 = t_univ(1.0)\n",
    "    t2 = t_univ(0.0)\n",
    "    #print quad(sens, 0.0, 1.0)[0]\n",
    "    #N_merge = np.sum( np.logical_and(tf_samps > t1,tf_samps < t2))\n",
    "    t_crop = tf_samps[tf_samps < t2]\n",
    "    z_samps = np.vectorize(z_of_t)(t_crop)\n",
    "    N_merge = np.sum(Hubble(z_samps)*(1+z_samps)*sens(z_samps))/N_samps\n",
    "\n",
    "    #print \"   Estimate of normalisation of P(t):\", x1\n",
    "    \n",
    "    #But we need to check the normalisation of P(t) in the range t1 -> t2\n",
    "    \n",
    "    #Normalise PDF correctly, to obtain fraction of BHs which form binaries in a given interval of t_merge\n",
    "    tvals = np.logspace(9, 11, 500)\n",
    "    P_true = np.asarray([P_t(t, f, M_PBH, withHalo=False) for t in tvals])\n",
    "    f_bin = np.trapz(P_true, tvals)\n",
    "\n",
    "    res = f_bin*N_merge*calc_nPBH(f, M_PBH)/(quad(sens, 0.0, 1.0)[0])\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mrate = np.zeros(10)\n",
    "unmap = CalcLIGOMergerRate_unmapped2(1e-4, 300.0,sens_300Msun)\n",
    "remap = CalcLIGOMergerRate(1e-4, 300.0,sens_300Msun)\n",
    "\n",
    "print unmap, remap, (remap - unmap)/unmap\n",
    "\n",
    "#for i in tqdm(range(10)):\n",
    "#    Mrate[i] = CalcLIGOMergerRate_unmapped2(1e-2, 20.0,sens_20Msun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print merger_rate_z(0,1e-4, 300.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.figure()\n",
    "pl.hist(Mrate,50)\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upper limit on f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masses = [10.,20.,40.,100.,200.,300.]\n",
    "sensitivities = [sens_10Msun, sens_20Msun, sens_40Msun,\\\n",
    "                     sens_100Msun, sens_200Msun, sens_300Msun]\n",
    "R90_masses, R90_vals = np.loadtxt(\"data/R90_LIGO.txt\", unpack=True)\n",
    "\n",
    "f_UL = np.zeros_like(masses)\n",
    "\n",
    "for m_ind in tqdm(range(6)):\n",
    "\n",
    "    fvals = np.logspace(-4, 0, 11)\n",
    "\n",
    "    Rvals = np.zeros_like(fvals)\n",
    "\n",
    "    for i in range(len(fvals)):\n",
    "        Rvals[i] = CalcLIGOMergerRate(fvals[i], masses[m_ind],sensitivities[m_ind])\n",
    "\n",
    "    np.savetxt('data/LIGO_rate_' + str(int(masses[m_ind]))+'.txt', zip(fvals, Rvals), header='Merger rate for M_PBH/M_sun = '+str(masses[m_ind]) + '\\nColumns: f_PBH, R [Gpc^-3 yr^-1]')\n",
    "\n",
    "    f_inverse = interp1d(Rvals, fvals)\n",
    "\n",
    "    R_upper = R90_vals[np.where(R90_masses==masses[m_ind])]\n",
    "\n",
    "    print \"90% UL is f = \", f_inverse(R_upper)\n",
    "\n",
    "    f_UL[m_ind] = f_inverse(R_upper)\n",
    "    \n",
    "    pl.figure()\n",
    "    pl.loglog(fvals, Rvals)\n",
    "\n",
    "    pl.axhline(R_upper, linestyle='--', color='k')\n",
    "\n",
    "    pl.title(r\"$M_\\mathrm{PBH} = \" + str(masses[m_ind]) +\"\\,M_\\odot$\")\n",
    "    pl.ylabel(r\"$\\mathcal{R}$ [$\\mathrm{Gpc}^{-3}\\,\\mathrm{yr}^{-1}$]\")\n",
    "    pl.xlabel(r\"$f_\\mathrm{PBH}$\")\n",
    "    pl.tight_layout()\n",
    "\n",
    "    pl.savefig('MergerRate_M=' + str(int(masses[m_ind])) + '.pdf')\n",
    "\n",
    "    pl.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('data/LIGO_limit_f.txt', zip(masses, f_UL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the LIGO upper limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#R90_LIGO = 2.303/VT\n",
    "\n",
    "#np.savetxt(\"data/R90_LIGO.txt\", zip(M_list, R90_LIGO), header=\"90% UL on merger rate density from LIGO\\nColumns: M_BH [M_solar], R_90% [Gpc^-3 yr^-1]\")\n",
    "\n",
    "M1 = np.linspace(masses[0], masses[-1])\n",
    "\n",
    "print f_UL\n",
    "\n",
    "lf_interp = interp1d(np.log10(masses), np.log10(f_UL), kind='linear')\n",
    "\n",
    "pl.figure(figsize=(7,5))\n",
    "pl.fill_between(M1, 10**lf_interp(np.log10(M1)), 1e3, color='blue', alpha=0.20)\n",
    "pl.loglog(masses, f_UL, 's', markersize=5.0)\n",
    "pl.xlabel(r\"$M_\\mathrm{PBH} \\,[M_\\odot]$\")\n",
    "pl.ylabel(r\"$f_\\mathrm{PBH}$\")\n",
    "pl.title(\"PBH fraction upper limit\", fontsize=14.0)\n",
    "\n",
    "pl.ylim(1e-4, 1)\n",
    "pl.xlim(1, 1e3)\n",
    "\n",
    "pl.tight_layout()\n",
    "pl.savefig('LIGO_UL.pdf')\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
